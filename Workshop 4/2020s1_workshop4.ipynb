{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements Of Data Processing (2020S1) - Week 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions \n",
    "Regular expressions allow you to match patterns in strings, rather than matching exact characters.  \n",
    "For example, \n",
    "if I wished to find all phone numbers of the form (03) xxxx xxxx, where x is some arbitrary digit, \n",
    "I could use a regular expression like this: \n",
    "    \n",
    "\\(03\\) \\d\\d\\d\\d \\d\\d\\d\\d\n",
    "\n",
    "*or*\n",
    "\n",
    "\\(03\\) \\d{4} \\d4}    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **re** library in python allows you to use regular expressions.  It provides a number of useful functions, \n",
    "including:\n",
    "    \n",
    "***search*** - Searches for a particular pattern in a string\n",
    "\n",
    "***findall*** - Finds all substrings that match a particular pattern\n",
    "\n",
    "***sub*** - Replaces substrings that match a particular pattern with a new substring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### This example looks for phone numbers that match the format above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone number found\n"
     ]
    }
   ],
   "source": [
    "#This examples looks for phone numbers that match the format above\n",
    "import re\n",
    "\n",
    "string = r'Name: Chris, ph: (03) 9923 1123, comments: this is not my real number'\n",
    "pattern = r'\\(03\\) \\d{4} \\d{4,4}'\n",
    "\n",
    "if re.search(pattern, string) :\n",
    "    print(\"Phone number found\")\n",
    "else :\n",
    "    print(\"Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 1 </span>\n",
    "\n",
    "Modify the example above so that it will also find phone numbers starting with 03 that:\n",
    "    \n",
    "- are missing brackets and/or\n",
    "- instead of a space, use hyphens,  backslashes and/or spaces.\n",
    "\n",
    "Your program should match all elements in ***strings*** in the code segment below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This examples looks for phone numbers that match the format above\n",
    "import re\n",
    "strings = [\n",
    "    r'Name: Chris, ph: (03) 9923 1123, comments: this is not my real number',\n",
    "    r'Name: John, ph: 03-9923-1123, comments: this might be an old number',\n",
    "    r'Name: Sara, phone: (03)-9923-1123, comments: there is data quality issues, so far, three people sharig the same number',\n",
    "    r'Name: Christopher, ph: (03)\\-9923 -1123, comments, is this the same Chris in the first record?'\n",
    "]\n",
    "\n",
    "#change this line\n",
    "#pattern = r'\\(03\\) \\d{4} \\d{4,4}'\n",
    "\n",
    "\n",
    "for s in strings:\n",
    "    if re.search(pattern, s) :\n",
    "        print(\"Phone number found\")\n",
    "    else :\n",
    "        print(\"Not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 2 </span>\n",
    "\n",
    "Write a program that will remove all leading zeros from an IP address\n",
    "    \n",
    "For example, 0216.08.094.102 should become 216.8.94.196\n",
    "\n",
    "Your program should match all elements in ***strings*** in the code segment below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2: Write a program that will remove all leading zeros from an IP address\n",
    "#For example, 0216.08.094.102 should become 216.8.94.196\n",
    "import re\n",
    "\n",
    "ip_addr = '0216.08.094.102'\n",
    "\n",
    "#change this line\n",
    "revised_addr = ip_addr\n",
    "\n",
    "\n",
    "print(revised_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping ##\n",
    "The BeautifulSoup library can be used to scrape data from a web page for processing and analysis.  You can find out more about BeautifulSoup at https://www.crummy.com/software/BeautifulSoup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This example extracts tennis scores from the 2019 ATP Tour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This example extracts tennis scores from the 2019 ATP Tour\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#Specify the page to download\n",
    "u = 'https://en.wikipedia.org/wiki/2019_ATP_Tour'\n",
    "page = requests.get(u)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "#Locate the section we're interested in.  Here we are after the second table after the 'ATP_ranking id'\n",
    "section = soup.find(id='ATP_ranking')\n",
    "results = section.findNext('table').findNext('table')\n",
    "\n",
    "#Iterate through all rows in the resultant table\n",
    "rows = results.find_all('tr')\n",
    "\n",
    "i = 0\n",
    "records = []\n",
    "\n",
    "#for row in rows[1:2]:\n",
    "#    cells = row.find_all('th')\n",
    "#    print(\"{0}, {1}, {2} ,{3}\".format(cells[0].text.strip(), cells[1].text.strip(), cells[2].text.strip(), cells[3].text.strip()))\n",
    "#    # column headers are #, Player, Points, Tours\n",
    "    \n",
    "for row in rows[2:]:\n",
    "    cells = row.find_all('td')\n",
    "    record = []\n",
    "    #print(\"{0}::{1}::{2}::{3}\".format(cells[0].text.strip(), cells[1].text.strip(), cells[2].text.strip(), cells[3].text.strip()))\n",
    "    # column value: 1::Rafael Nadal (ESP)::9,585::12\n",
    "    \n",
    "    #Removes junk characters from string and stores the result\n",
    "    ranking = int(unicodedata.normalize(\"NFKD\", cells[0].text.strip()))\n",
    "    record.append(int(ranking))\n",
    "    \n",
    "    player = unicodedata.normalize(\"NFKD\", cells[1].text.strip())\n",
    "    #Removes the country from the player name, removing surrounding whitespaces.\n",
    "    player_name = re.sub('\\(.*\\)', '', player).strip()\n",
    "    #print(player_name)\n",
    "    record.append(player_name)\n",
    "\n",
    "    #Remove the thousands separator from the points value and store as an integer\n",
    "    points = unicodedata.normalize(\"NFKD\", cells[2].text.strip())\n",
    "    record.append(int(re.sub(',', '', points)))\n",
    "    \n",
    "    # number of tours: integer type\n",
    "    tours = unicodedata.normalize(\"NFKD\", cells[3].text.strip())\n",
    "    record.append(int(tours))\n",
    "    \n",
    "    #Store the country code separately\n",
    "    country_code = re.search('\\((.*)\\)', player).group(1)\n",
    "    record.append(country_code)\n",
    "    #print(record)\n",
    "    #[1, 'Rafael Nadal', 9585, 12, 'ESP']\n",
    "    records.append(record)\n",
    "    i = i+1\n",
    "\n",
    "column_names = [\"ranking\", \"player\", \"points\", \"tours\", \"country\"]\n",
    "tennis_data = pd.DataFrame(records, columns = column_names)\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.bar(tennis_data['player'], tennis_data['points'])\n",
    "plt.ylabel('points')\n",
    "plt.title(\"ATP Tour - player points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note on *unicodedata.normalize()*\n",
    "\n",
    "Web pages commonlu uses uncode encoding.\n",
    "\n",
    "Most ASCII characters are printable characters of the english alphabet such as abc, ABC, 123, ?&!, etc., represented as a number between 32 and 127.\n",
    "    \n",
    "Unicode represents most written languages and still has room for even more; this\n",
    "includes typical left-to-right scripts like English and even right-to-left scripts like Arabic. Chinese, Japanese, and the many other variants are also represented within Unicode\n",
    "ASCII has its equivalent within Unicode.\n",
    "\n",
    "\n",
    "\n",
    "In Unicode, several characters can be expressed in various way. \n",
    "For example, the character U+00C7 (LATIN CAPITAL LETTER C WITH CEDILLA) \n",
    "can also be expressed as the sequence U+0043 (LATIN CAPITAL LETTER C) U+0327 (COMBINING CEDILLA).\n",
    "\n",
    "The Unicode standard defines various normalization forms of a Unicode string, \n",
    "based on the definition of canonical equivalence and compatibility equivalence. \n",
    "\n",
    "The function ***unicodedata.normalize(\"NFKD\", unistr)*** \n",
    "will apply the compatibility decomposition, i.e. replace all compatibility characters with their equivalents.\n",
    "\n",
    "#### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unistr = u'\\u2460'\n",
    "print(\"{0} is the equivalent character of {1}\".format(unicodedata.normalize('NFKD', unistr), unistr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 3 </span>\n",
    "\n",
    "Produce a graph similar to the example above for the **2019 ATP Doubles Scores**.\n",
    "\n",
    "*First locate the section we're interested in.*\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution of exercise 3\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Specify the page to download\n",
    "u = 'https://en.wikipedia.org/wiki/2019_ATP_Tour'\n",
    "page = requests.get(u)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "## complete the program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web crawling ##\n",
    "\n",
    "This example implements a simplified Web crawler that traverses the site \n",
    "**[http://books.toscrape.com/](http://books.toscrape.com/)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "page_limit = 20\n",
    "\n",
    "#Specify the initial page to crawl\n",
    "base_url = 'http://books.toscrape.com/'\n",
    "seed_item = 'index.html'\n",
    "\n",
    "seed_url = base_url + seed_item\n",
    "page = requests.get(seed_url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "visited = {}; \n",
    "visited[seed_url] = True\n",
    "pages_visited = 1\n",
    "print(seed_url)\n",
    "\n",
    "#Remove index.html\n",
    "links = soup.findAll('a')\n",
    "seed_link = soup.findAll('a', href=re.compile(\"^index.html\"))\n",
    "#to_visit_relative = list(set(links) - set(seed_link))\n",
    "to_visit_relative = [l for l in links if l not in seed_link]\n",
    "\n",
    "\n",
    "# Resolve to absolute urls\n",
    "to_visit = []\n",
    "for link in to_visit_relative:\n",
    "    to_visit.append(urljoin(seed_url, link['href']))\n",
    "\n",
    "    \n",
    "#Find all outbound links on succsesor pages and explore each one \n",
    "while (to_visit):\n",
    "    # Impose a limit to avoid breaking the site \n",
    "    if pages_visited == page_limit :\n",
    "        break\n",
    "        \n",
    "    # consume the list of urls\n",
    "    link = to_visit.pop(0)\n",
    "    print(link)\n",
    "\n",
    "    # need to concat with base_url, an example item <a href=\"catalogue/sharp-objects_997/index.html\">\n",
    "    page = requests.get(link)\n",
    "    \n",
    "    # scarping code goes here\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    \n",
    "    # mark the item as visited, i.e., add to visited list, remove from to_visit\n",
    "    visited[link] = True\n",
    "    to_visit\n",
    "    new_links = soup.findAll('a')\n",
    "    for new_link in new_links :\n",
    "        new_item = new_link['href']\n",
    "        new_url = urljoin(link, new_item)\n",
    "        if new_url not in visited and new_url not in to_visit:\n",
    "            to_visit.append(new_url)\n",
    "        \n",
    "    pages_visited = pages_visited + 1\n",
    "\n",
    "print('\\nvisited {0:5d} pages; {1:5d} pages in to_visit'.format(len(visited), len(to_visit)))\n",
    "#print('{0:1d}'.format(pages_visited))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 4 </span>\n",
    "\n",
    "The code above can easily be end up stuck in a crawler trap.  \n",
    "Explain three ways this could occur and suggest possible solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 5 </span>\n",
    "\n",
    "Modify the code above to print the titles of as many books as can be found within the page_limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution to exercise 5\n",
    "# Modify the code above to print the titles of as many books as can be found within the page_limit\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# complete the program\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing ##\n",
    "The ***nltk*** library provides you with tools for natural language processing, including tokenizing, stemming and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jaypr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", 22\n",
      ". 10\n",
      "-- 7\n",
      "dedic 6\n",
      "nation 5\n",
      "live 4\n",
      "great 3\n",
      "It 3\n",
      "dead 3\n",
      "us 3\n",
      "shall 3\n",
      "peopl 3\n",
      "new 2\n",
      "conceiv 2\n",
      "men 2\n",
      "war 2\n",
      "long 2\n",
      "We 2\n",
      "gave 2\n",
      "consecr 2\n",
      "the 2\n",
      "far 2\n",
      "rather 2\n",
      "devot 2\n",
      "four 1\n",
      "score 1\n",
      "seven 1\n",
      "year 1\n",
      "ago 1\n",
      "father 1\n",
      "brought 1\n",
      "forth 1\n",
      "contin 1\n",
      "liberti 1\n",
      "proposit 1\n",
      "creat 1\n",
      "equal 1\n",
      "now 1\n",
      "engag 1\n",
      "civil 1\n",
      "test 1\n",
      "whether 1\n",
      "endur 1\n",
      "met 1\n",
      "battle-field 1\n",
      "come 1\n",
      "portion 1\n",
      "field 1\n",
      "final 1\n",
      "rest 1\n",
      "place 1\n",
      "might 1\n",
      "altogeth 1\n",
      "fit 1\n",
      "proper 1\n",
      "but 1\n",
      "larger 1\n",
      "sens 1\n",
      "hallow 1\n",
      "ground 1\n",
      "brave 1\n",
      "struggl 1\n",
      "poor 1\n",
      "power 1\n",
      "add 1\n",
      "detract 1\n",
      "world 1\n",
      "littl 1\n",
      "note 1\n",
      "rememb 1\n",
      "say 1\n",
      "never 1\n",
      "forget 1\n",
      "unfinish 1\n",
      "work 1\n",
      "fought 1\n",
      "thu 1\n",
      "nobli 1\n",
      "advanc 1\n",
      "task 1\n",
      "remain 1\n",
      "honor 1\n",
      "take 1\n",
      "increas 1\n",
      "caus 1\n",
      "last 1\n",
      "full 1\n",
      "measur 1\n",
      "highli 1\n",
      "resolv 1\n",
      "die 1\n",
      "vain 1\n",
      "god 1\n",
      "birth 1\n",
      "freedom 1\n",
      "govern 1\n",
      "perish 1\n",
      "earth 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jaypr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# if running the first time with errors:\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "#\n",
    "porterStemmer = PorterStemmer()\n",
    "\n",
    "speech = 'Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth.'\n",
    "wordList = nltk.word_tokenize(speech)\n",
    "\n",
    "# run the line to download it the first time:\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "filteredList = [w for w in wordList if not w in stopWords]\n",
    "\n",
    "wordDict = {}\n",
    "for word in filteredList:\n",
    "    stemWord = porterStemmer.stem(word)\n",
    "    if stemWord in wordDict : \n",
    "        wordDict[stemWord] = wordDict[stemWord] +1\n",
    "    else :\n",
    "        wordDict[stemWord] = 1\n",
    "\n",
    "wordDict = {k: v for k, v in sorted(wordDict.items(), key=lambda item: item[1], reverse=True)}\n",
    "for key in wordDict : print(key, wordDict[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Exercise 6 </span>\n",
    "\n",
    "Modify the example above to use a WordNet Lemmatizer instead of a porter stemmer.\n",
    "\n",
    "Comment on the differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jaypr/nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jaypr/nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-8c997707d0b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#porterStemmer = PorterStemmer()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwnl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dogs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mspeech\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    121\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    697\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\jaypr/nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\jaypr\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Solution to Exercise 6: \n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "# if running the first time with errors:\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "#porterStemmer = PorterStemmer()\n",
    "\n",
    "print(wnl.lemmatize('dogs'))\n",
    "\n",
    "speech = 'Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. Now we are engaged in a great civil war, testing whether that nation, or any nation so conceived and so dedicated, can long endure. We are met on a great battle-field of that war. We have come to dedicate a portion of that field, as a final resting place for those who here gave their lives that that nation might live. It is altogether fitting and proper that we should do this. But, in a larger sense, we can not dedicate -- we can not consecrate -- we can not hallow -- this ground. The brave men, living and dead, who struggled here, have consecrated it, far above our poor power to add or detract. The world will little note, nor long remember what we say here, but it can never forget what they did here. It is for us the living, rather, to be dedicated here to the unfinished work which they who fought here have thus far so nobly advanced. It is rather for us to be here dedicated to the great task remaining before us -- that from these honored dead we take increased devotion to that cause for which they gave the last full measure of devotion -- that we here highly resolve that these dead shall not have died in vain -- that this nation, under God, shall have a new birth of freedom -- and that government of the people, by the people, for the people, shall not perish from the earth.'\n",
    "wordList = nltk.word_tokenize(speech)\n",
    "\n",
    "# run the line to download it the first time:\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "\n",
    "filteredList = [w for w in wordList if not w in stopWords]\n",
    "\n",
    "wordDict = {}\n",
    "for word in filteredList:\n",
    "    stemWord = wordnet.lemmatize(word)\n",
    "    if stemWord in wordDict : \n",
    "        wordDict[stemWord] = wordDict[stemWord] +1\n",
    "    else :\n",
    "        wordDict[stemWord] = 1\n",
    "\n",
    "wordDict = {k: v for k, v in sorted(wordDict.items(), key=lambda item: item[1], reverse=True)}\n",
    "for key in wordDict : print(key, wordDict[key])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
